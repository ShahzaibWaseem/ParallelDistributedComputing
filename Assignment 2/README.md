 

Assignment 2 \[100 Points\]
---------------------------

In this assignment, we will continue to learn how to parallelize programs using C++11 threads. There are two problems in this assignment, and for each problem you are provided the serial C++ implementation, the expected parallelization strategy, and the expected output to be generated by your parallel solution.

Before starting this assignment, you should have completed [Tutorial 1](https://www.cs.sfu.ca/~keval/teaching/cmpt770/fall21/slurm_tutorial/release/index.html) which walks you through how to use our servers for your code development.

Performance of a program is often sensitive to the way in which data is laid out in memory. [Tutorial 2](https://www.cs.sfu.ca/~keval/teaching/cmpt770/fall21/data_layout/release/index.html) explains how data layout in memory can affect performance. You should ensure your solutions do not suffer from false sharing, and are not limited by poor choice of data layout.

### General Instructions

1.  You are provided with the serial version of all the programs [here](https://www.cs.sfu.ca/~keval/teaching/cmpt770/fall21/assignments/assignment2/assignment2.tar.gz). To run a program (e.g., `page_rank_parallel.cpp`), follow the steps below:
    *   Run `make page_rank_parallel`. This creates a binary file called `page_rank_parallel`.
    *   Create a slurm job to run the binary file using the following command: `./page_rank_parallel --nWorkers 4 --nIterations 10 --inputFile absolute_path_of_input_graph`
    *   Detailed descriptions for the command-line arguments is provided below.
2.  All parallel programs should have the command-line argument `--nWorkers` to specify the number of threads for the program. Example: `--nWorkers 4`.
3.  **While testing your solutions, make sure that `cpus-per-task` is correctly specified in your slurm config file based on your requirement.**
4.  You will be asked to print the time spent by different threads on specific code regions. The time spent by any code region can be computed as follows:
    
        timer t1;
        t1.start();
        /* ---- Code region whose time is to be measured --- */
        double time_taken = t1.stop();
        
    
5.  Sample outputs for all the programs can be found in `sample_outputs` directory. **Programs will be evaluated and graded automatically. Please make sure that your program output strictly follows the sample output format.**
6.  We have provided test scripts for you to quickly test your solutions during your development process. You can test your code using the test script available at `/scratch/assignment2/test_scripts/`. Note that these test scripts only validate the output formats, and a different evaluation script will be used for grading the assignments. **Important: You should use slurm when performing these and other tests**. The test scripts under `/scratch/assignment2/test_scripts/` folder test for up to 4 threads; make sure `--cpus-per-task=4` is set in your slurm job.
    
        $ ls /scratch/assignment2/test_scripts/*tester.pyc
        page_rank_lock_tester.pyc page_rank_atomic_tester.pyc
        
    
7.  The programs operate on graph datasets. Sample input graphs are available at `/scratch/input_graphs/` on the compute nodes (note they are present on the compute nodes only, and hence you can access them via slurm only).
    
        $ ls /scratch/input_graphs/*.cs*
        lj.csc  lj.csr  roadNet-CA.csc  roadNet-CA.csr  test_25M_50M.csc  test_25M_50M.csr
        
    
    If you are interested in checking out the original graph datasets (this is not required to solve the assignment), you can find them [here](https://snap.stanford.edu/data/index.html).
8.  If you'd like to test your solution with more graph datasets, you can create your own simple graphs as follows:
    *   Create a file called `testGraph.txt` with the list of edges (one edge on each line in "`<source> <destination>`" form) in the graph. For example,
        
            1 2
            2 3
            
        
    *   Run `/scratch/input_graphs/SNAPtoBinary testGraph.txt testGraphConverted`. This will create `testGraphConverted.csr` and `testGraphConverted.csc` files which are [CSR and CSC](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)) representations of the graph.
    *   To use the graphs in your solutions, use the command line argument `--inputFile "testGraphConverted"`.

### 1\. PageRank with Locks \[60 Points\]

Given a graph, the [PageRank](https://en.wikipedia.org/wiki/PageRank) of a vertex `v` is computed as:

    pagerank[v] = (1 - DAMPING) + (DAMPING * Σ(pagerank[u] / out_degree[u]))
    

where `DAMPING` is a constant set to `0.85`, and `Σ` represents summation over all incoming edges `(u, v)` of `v`.

Note that `v` might be an incoming neighbor for some other vertex `w`, and hence, we cannot simply overwrite `pagerank[v]` before `pagerank[w]` gets computed. To do so, we separate the pagerank values on the left of the equation with the pagerank values on the right of the equation, as shown here:

    pagerank_next[v] = (1 - DAMPING) + (DAMPING * Σ (pagerank_curr[u] / out_degree[u]))
    

where `pagerank_next[v]` is the new pagerank of `v` and `pagerank_curr[u]` is the old pagerank of `u`.

The pagerank for all the vertices in the graph is computed multiple times for certain number of iterations. In this problem, we will look at an implementation of pagerank where each vertex `u` pushes its pagerank value to its outgoing neighbors, after which, the cumulative value received by each vertex gets used to compute pagerank of the vertex. The serial code shown below repeats this process for `max_iters` iterations.

       // Initialization
       for (uintV i = 0; i < n; i++) {
           pr_curr[i] = 1.0;
           pr_next[i] = 0.0;
       }
    
       // ----------------------------------------------------------------
       for (int iter = 0; iter < max_iters; iter++) {
           for (uintV u = 0; u < n; u++) {
               uintE out_degree = g.vertices_[u].getOutDegree();
               for (uintE i = 0; i < out_degree; i++) {
                   uintV v = g.vertices_[u].getOutNeighbor(i);
                   pr_next[v] += (pr_curr[u] / out_degree);
               }
           }
           for (uintV u = 0; u < n; u++) {
               // pr_next[u] contains the cumulative value received by u          
               pr_next[u] = (1 - DAMPING) + (DAMPING * pr_next[u]);  
               // Update pr_curr and reset pr_next for the next iteration
               pr_curr[u] = pr_next[u];
               pr_next[u] = 0.0;
           }
       }
       // ----------------------------------------------------------------
       
    

Our goal is to parallelize the above algorithm. Specifically, we are interested in parallelizing the for loop demarcated by comments (`// ---`) such that each thread works on a sub-graph. Below is the pseudo-code showing the logic of our parallel solution:

        Create T threads
        for(i=0; i<max_iterations; i++) {
            for each thread in parallel {
                for each vertex 'u' allocated to the thread {
                    for vertex 'v' in outNeighbor(u)
                        next_page_rank[v] += (current_page_rank[u]/outdegree[u]) 
                }
            }
                
            for each thread in parallel {
                for each vertex 'v' allocated to the thread {
                    compute the new_pagerank using the accumulated values in next_page_rank[v].
                    current_page_rank[v] = new_pagerank
                    Reset next_page_rank[v] to 0
                }
            }
        }
    

**Key things to note:**

1.  Observe the variables shared by multiple threads. **Ensure that access to shared resources is properly synchronized using locks**. Use [std::mutex](https://en.cppreference.com/w/cpp/thread/mutex) as needed.
2.  Observe the separation between the two for loops. Only after all the threads have processed the first for loop, the second for loop should be processed by each thread. You will need a barrier to achieve this. We have provided `CustomBarrier` in the `utils.h` which you can use as shown below:
    
        CustomBarrier my_barrier(4);   //Create a barrier object. 4 --> number of workers/threads
        // Share this my_barrier object with all the threads
        // ---
            // Inside the thread function, wait on the barrier as follows:
            my_barrier.wait();
        // ---
        
    

The serial implementation is available in `page_rank.cpp`. You have to parallelize the given serial implementation using C++11 threads and locks using [std::mutex](https://en.cppreference.com/w/cpp/thread/mutex).

The output of your program can be verified by comparing the sum of all the pagerank values with that generated by the serial implementation. It is important to note that floating point multiplication is [not associative](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Accuracy_problems). So, there may be minor variation in the pagerank values compared to serial implementation. For quick verification, we have also provided an integer version of the program. To run the integer version of PageRank, use the flag `USE_INT=1` during `make` as follows:

        $ make USE_INT=1 page_rank
    

Note that the floating point based implementation is the default implementation (i.e., doesn't require any flags).

Your parallel solution must satisfy the following:

1.  The file should be named `page_rank_parallel.cpp` and should support the following command-line parameters:
    *   `--nWorkers`: The number of worker threads.
    *   `--nIterations`: The number of iterations (similar to the serial code).
    *   `--inputFile`: The absolute path to the input graph file (similar to the serial code).
2.  Your parallel solution must output the following information:
    *   Total number of threads used.
    *   For each thread: the time taken to compute pageranks (your threads should be numbered between `[0, T)`).
    *   The sum of pageranks of all vertices.
    *   The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
3.  The sample console output can be found at `sample_outputs/page_rank.output`.

Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows (remember to run this via slurm):

        $ python /scratch/assignment2/test_scripts/page_rank_lock_tester.pyc --execPath=<absolute path of page_rank_parallel>
    

### 2\. PageRank with Atomics \[40 Points\]

Here we will improve the above PageRank solution by eliminating `std::mutex` locks. Recall that atomic operations like [Compare-and-Swap](https://en.wikipedia.org/wiki/Compare-and-swap) perform two memory operations (read and write) atomically. Such atomic operations allows us to eliminate locks in our PageRank solution. C++11 supports [std::atomics](https://en.cppreference.com/w/cpp/atomic/atomic) from which you should use [compare\_exchange](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange). You need to figure out how to use `compare_exchange` correctly such that your parallel solution becomes lockless.

Your parallel solution must satisfy the following:

1.  The file should be named `page_rank_parallel_atomic.cpp` and should support the following command-line parameters:
    *   `--nWorkers`: The number of worker threads.
    *   `--nIterations`: The number of iterations (similar to the serial code).
    *   `--inputFile`: The absolute path to the input graph file (similar to the serial code).
2.  Your parallel solution must output the following information:
    *   Total number of threads used.
    *   For each thread: the time taken to compute pageranks (your threads should be numbered between `[0, T)`).
    *   The sum of pageranks of all vertices.
    *   The total time taken for the entire execution (the code region to be timed is highlighted using comments in the serial code).
3.  The sample console output can be found at `sample_outputs/page_rank.output`.

Please note that the output format should strictly match the expected format (including "spaces" and "commas"). You can test your code using the test script as follows (remember to run this via slurm):

        $ python /scratch/assignment2/test_scripts/page_rank_atomic_tester.pyc --execPath=<absolute path of page_rank_parallel_atomic>
    

* * *

### Submission Guidelines

*   Make sure that your solutions folder has the following files and sub-folders. Let's say your solutions folder is called `my_assignment2_solutions`. It should contain:
    
    *   `core/` -- The folder containing all core files. It is already available in the assignement package. Do not modify it or remove any files.
    *   `Makefile` -- Makefile for the project. This file should not be changed.
    *   `page_rank_parallel.cpp`
    *   `page_rank_parallel_atomic.cpp`
*   To create the submission file, follow the steps below:
    
    1.  Enter in your solutions folder, and remove all the object/temporary files.
        
            $ cd my_assignment2_solutions/
            $ make clean
            
        
    2.  Create the tar.gz file.
        
            $ tar cvzf assignment2.tar.gz *
            
        
        which creates a compressed tar ball that contains the contents of the folder.
    3.  Validate the tar ball using the `submission_validator.pyc` script.
        
            $ python /scratch/assignment2/test_scripts/submission_validator.pyc --tarPath=assignment2.tar.gz
            
        
*   Submit via [CourSys](https://courses.cs.sfu.ca/) by the deadline posted there.
    

* * *

Copyright © 2020 Keval Vora. All rights reserved.